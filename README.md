# OPAL (Ordinary People Accelerating Learning)

*OPAL* is design tool that uses artificial neural networks (ANN) to automate the design and optimization of other ANNs: neural networks designing neural networks. *OPAL* is available for download as a compiled library (developed in python3) for educational and research purposes. Source code is available for educational and research purposes upon request at [opal@ece.mcgill.ca](mailto:opal@ece.mcgill.ca); for commercialization options, [contact us](mailto:opal@ece.mcgil.ca).

*OPAL* is a joint effort between the Reliable Silicon Systems Lab (RSSL) directed by Professor [Brett H. Meyer](http://rssl.ece.mcgill.ca) and the Integrated Systems for Information Processor (ISIP) Lab directed by Professor [Warren J. Gross](http://www.isip.ece.mcgill.ca). If you use *OPAL* in your research, please cite [our paper](https://arxiv.org/abs/1611.02120):
> Sean C. Smithson, Guang Yang, Warren J. Gross, and Brett H. Meyer, "Neural networks designing neural networks: Multi-objective hyper-parameter optimization," 2016 International Conference On Computer Aided Design (ICCADâ€™16), November 2016.


## Overview

The design of ANN hyper-parameters has long been considered unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques. Most current approaches focus on optimizing accuracy, with little regard to the resulting computational resource requirements. *OPAL* uses response surface modelling to learn the relationship between ANN hyper-parameters and network accuracy, and further employs a model of the cost of network implementation (e.g., in C or CUDA) to direct ANN design space exploration and expose the best accuracy-cost trade-offs possible.


## Usage

The tool can be used by simply executing the *OPAL* terminal script followed by a configuration file as input (e.g. ./OPAL example.cfg). The outputs generated by the tool are in a human readable plain text format, containing the complete exploration results (input hyper-parameter and resulting cost/accuracy  pairs) as well as the Pareto-optimal results.


## Configuration Files

The general structure of the configuration files is split up into five distinct sections: [General], [DSE], [Cost], [Parameters], and [RSM]. The possible parameters and values for each section are outlined below.


### [General] Section:

The [General] section contains the configuration options pertinent to the input and output file locations, and type of design problem. The possible input to this section are:

Parameter | Description | Possible Values
--------- | ----------- | ---------------
verbose | Whether to print out details to standard output or not | *True* or *False*
experiment_name | Name that will be included in the results files generated | Any value that can be valid file name
results_dir | Location directory where the results are to be stored in (must exist) | Any value that is a valid directory name
dataset | Name of the dataset used | Can be *GENERIC* if working with a supplied dataset in formatted .npy files. Otherwise, standard image datasets which can be used are *MNIST*, *CIFAR-10*, or *SVHN* using their distributed files.
dataset_dir | If the dataset is not *GENERIC*, then this is the directory where the files are stored | Any value that is a valid directory name
train_input_filename | File name of the training set inputs, the arrays must be structured such that the size of the first dimension is the number of samples | Any value that is a valid file name
train_output_filename | Same as above, but the training outputs | Any value that is a valid file name
test_input_filename | Same as above, but the testing inputs | Any value that is a valid file name
test_output_filename | Same as above, but the testing outputs | Any value that is a valid file name
network_type | Type of neural network being designed | *MLP* or *2DCNN*


### [DSE] Section:

The [DSE] section contains the details of the design space exploration algorithm configuration, including how long to run for. The possible input to this section are:

Parameter | Description | Possible Values
--------- | ----------- | ---------------
max_samples | The maximum samples predicted from the design space; if the true Pareto-optimal front is found by the tool, without a value for *max_samples* the algorithm will run forever re-sampling the same solutions over and over. | Any numerical values, notations such as *1e6* will be converted to integers internally.
max_iterations | The maximum number of solutions that will be trained and tested, this is what determines the algorithm run time. | This must be a valid integer value.
initial_iterations | | 
storage_period | | 
sigma | | 
alpha | | 


### [Cost] Section:

The [Cost] section.

Parameter | Description | Possible Values
--------- | ----------- | ---------------
weight_cost |  | 
mac_cost |  | 
max_cost |  | 


### [Parameters] Section:

The [Parameters] section.

Parameter | Description | Possible Values
--------- | ----------- | ---------------
 |  | 
 |  | 


### [RSM] Section:

The [RSM] section.

Parameter | Description | Possible Values
--------- | ----------- | ---------------
network_type |  | 
loss |  | 
epochs |  | 
learning_rate |  | 
learning_alg |  | 
fc_layers |  | 
fc_dropouts |  | 
fc_activation |  | 
conv_filters |  | 
conv_kernels |  | 
conv_strides |  | 
conv_pads |  | 
conv_pools |  | 
conv_dropouts |  | 
conv_activation |  | 
out_activation |  | 

Note that care should be taken when writing the [Parameters] and [RSM] sections of configuration files, as the parameter strings are executed by the Python interpreter. This ability was added for convenience when specifying parameters with many values to be explored; it is a given that the tool should not be run with root privileges.


## Dependencies

Aside from a working Python installation (>= 3.4), the following libraries are required:
 - [numpy](http://www.numpy.org/)
 - [scipy](http://www.scipy.org/)
 - [matplotlib](http://matplotlib.org/)
 - [theano](http://www.deeplearning.net/software/theano/)
 - [lasagne](https://github.com/Lasagne/Lasagne)

The provided AMD64 builds have been tested on Ubuntu versions 14.04 and 16.04, with numpy 1.11.3, scipy 0.18.1, matplotlib 2.0.0, theano 0.9.0b1, and lasagne 0.2.dev1. However, the release may still function with earlier versions.


<!-- ## Frequently Asked Questions -->

