[General]
verbose = True
experiment_name = example_CNN
results_dir = ./results/
dataset = MNIST
dataset_dir = ./Datasets/MNIST
network_type = 2DCNN


[DSE]
max_samples = 1e8
max_iterations = 200
initial_iterations = 2
storage_period = 10
sigma = 3
alpha = 1e-4


[Cost]
weight_cost = 1.0
mac_cost = 0.0072
max_cost = 5e4


[Parameters]
conv_depth = np.arange(1, 10, dtype='int32')
fc_depth = np.int32([0, 1])
fc_layers = np.int32(2**np.arange(start=5, stop=9))
fc_layers_local = ['fc_depth']
conv_filters = np.int32(2**np.arange(start=4, stop=7))
conv_filters_local = ['conv_depth']
conv_activation = ['ReLU']
fc_activation = ['ReLU']
out_activation = ['softmax']
conv_pooltype = ['max2d']
conv_pools = [1, 2]
conv_pools_local = ['conv_depth']
conv_kernelx = [3, 5]
conv_kernely = [3, 5]
conv_kernelx_local = ['conv_depth']
conv_kernely_local = ['conv_depth']
conv_strides = [1, 2, 3]
conv_strides_local = ['conv_depth']
learning_rate = np.logspace(start=np.log10(0.001), stop=np.log10(0.2), num=15, dtype='float32')
epochs = [10]
batchsize = [200]
updates = ['ADAM']
loss = ['categorical_crossentropy']


[RSM]
network_type = MLP
fc_activation = ReLU
out_activation = linear
epochs = 100
fc_layers = np.ones(2) * 512
fc_dropouts = None
learning_rate = 0.1
learning_alg = SGD
loss = MSE


