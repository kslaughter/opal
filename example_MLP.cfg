[General]
verbose = True
experiment_name = example_MLP
results_dir = ./results/
dataset = MNIST
dataset_dir = ./Datasets/MNIST
network_type = MLP


[DSE]
max_samples = 1e8
max_iterations = 100
initial_iterations = 2
storage_period = 10
sigma = 3
alpha = 1e-4


[Cost]
weight_cost = 1.0
mac_cost = 0.0072


[Parameters]
fc_depth = np.int32([1, 2, 3])
fc_layers = np.unique(np.logspace(start=np.log10(10), stop=np.log10(500), num=100, dtype='int32'))
fc_layers_local = ['fc_depth']
learning_rate = np.logspace(start=np.log10(0.001), stop=np.log10(0.8), num=15, dtype='float32')
epochs = [10]
batchsize = [200]
fc_activation = ['ReLU', 'tanh']
out_activation = ['softmax']
updates = ['SGD']
loss = ['categorical_crossentropy']
fc_batchnorm = [True]
out_batchnorm = [True]
fc_dropout = [None]


[RSM]
network_type = MLP
fc_activation = ReLU
out_activation = linear
epochs = 100
fc_layers = np.int32(np.ones(2) * 150)
fc_dropouts = None
learning_rate = 0.1
learning_alg = SGD
loss = MSE

